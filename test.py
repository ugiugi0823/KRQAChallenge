import os, pytz, argparse
import json
import numpy as np
import pandas as pd
import re
import string
from collections import Counter
from tqdm import tqdm
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)
from peft import PeftModel, PeftConfig
from accelerate import Accelerator
from datetime import datetime

def normalize_answer(s):
    def remove_(text):
        ''' ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±° '''
        text = re.sub("'", " ", text)
        text = re.sub('"', " ", text)
        text = re.sub('ã€Š', " ", text)
        text = re.sub('ã€‹', " ", text)
        text = re.sub('<', " ", text)
        text = re.sub('>', " ", text)
        text = re.sub('ã€ˆ', " ", text)
        text = re.sub('ã€‰', " ", text)
        text = re.sub("\(", " ", text)
        text = re.sub("\)", " ", text)
        text = re.sub("'", " ", text)
        text = re.sub("'", " ", text)
        return text

    def white_space_fix(text):
        '''ì—°ì†ëœ ê³µë°±ì¼ ê²½ìš° í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´'''
        return ' '.join(text.split())

    def remove_punc(text):
        '''êµ¬ë‘ì  ì œê±°'''
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text):
        '''ì†Œë¬¸ì ì „í™˜'''
        return text.lower()

    return white_space_fix(remove_punc(lower(remove_(s))))

def generate_response(model, tokenizer, question_prompt):
    inputs = tokenizer(question_prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # ì‘ë‹µì—ì„œ "Answer:" ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    if "Answer:" in response:
        response = response.split("Answer:", 1)[1].strip()
        # ì²« ë¬¸ì¥ ë˜ëŠ” ìµœëŒ€ 30ë‹¨ì–´ë§Œ ìœ ì§€
        response = ' '.join(response.split()[:30])
        # ì¶”ê°€ ì§€ì‹œì‚¬í•­ì´ë‚˜ ë‹¤ìŒ ì§ˆë¬¸ì˜ ì‹œì‘ì  ì œê±°
        for cut_point in ["Question:", "Context:", "Instructions:", "âŠ™"]:
            if cut_point in response:
                response = response.split(cut_point, 1)[0]
    return response.strip()

def main(args):
    peft_model_path = args.model_path
    path_parts = peft_model_path.split('/')
    
    print("ğŸŒŠ"*40)
    model_id = f"{path_parts[5]}/{path_parts[6]}"
    print(model_id)
    print("ğŸŒŠ"*40)
    
    # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token
    
    # LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
    model = PeftModel.from_pretrained(model, peft_model_path)
    
    accelerator = Accelerator()
    model, tokenizer = accelerator.prepare(model, tokenizer)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_data = pd.read_csv('./data/test.csv')
    
    # ëª¨ë¸ ì¶”ë¡ 
    submission_dict = {}
    
    for index, row in tqdm(test_data.iterrows(), total=len(test_data)):
        try:
            context = row['context']
            question = row['question']
            id = row['id']
            if context is not None and question is not None:
                # ì§€ì‹œì‚¬í•­ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                instruction = "Please answer briefly in one or two sentences."
                question_prompt = f"Context: {context}\nQuestion: {question}\nInstructions: {instruction}\nAnswer:"
                answer = generate_response(model, tokenizer, question_prompt)
                
                # NaN ë˜ëŠ” null ê°’ ì²´í¬
                if pd.isna(answer) or answer == '' or answer is None:
                    print("ğŸ§ŠğŸ§ŠğŸ§ŠğŸ§Š Not good!ğŸ§ŠğŸ§ŠğŸ§ŠğŸ§Š")
                    # í•œ ë²ˆ ë” ì‹œë„
                    answer = generate_response(model, tokenizer, question_prompt)
                
                submission_dict[id] = answer if not pd.isna(answer) and answer != '' and answer is not None else 'No valid answer'
            else:
                submission_dict[id] = 'Invalid question or context'
        except Exception as e:
            print(f"Error processing question {id}: {e}")
            submission_dict[id] = 'Error occurred'
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    df = pd.DataFrame(list(submission_dict.items()), columns=['id', 'answer'])
    
    asia_timezone = pytz.timezone('Asia/Seoul')
    current_time = datetime.now(asia_timezone).strftime("%Y%m%d_%H%M%S")
    
    submission_folder = f'./data/sub/{model_id}'
    os.makedirs(submission_folder, exist_ok=True)
    peft_model_dir_name = os.path.basename(peft_model_path)
    submission_filename = os.path.join(submission_folder, f'sub_{current_time}_{path_parts[5]}_{path_parts[6]}_{path_parts[7]}_{peft_model_dir_name}.csv')
    
    df.to_csv(submission_filename, index=False)
    print(f"Submission file created: {submission_filename}")
    
    # NaN ë˜ëŠ” null ê°’ ì²´í¬
    nan_count = df['answer'].isna().sum()
    null_count = (df['answer'] == '').sum() + (df['answer'] == 'No valid answer').sum() + (df['answer'] == 'Error occurred').sum()
    
    print(f"NaN ê°’ì˜ ê°œìˆ˜: {nan_count}")
    print(f"Null ë˜ëŠ” ë¬´íš¨í•œ ê°’ì˜ ê°œìˆ˜: {null_count}")

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--model_path", type=str, default='str', help="Train Path")
    args = p.parse_args()
    main(args)
